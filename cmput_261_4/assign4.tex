\documentclass{article}
\newcommand{\duedate}{Thursday, December 7/2023}
\newcommand{\assignNum}{4}

\usepackage{enumerate}
\usepackage{booktabs}
\usepackage{amsmath, amsthm, amsfonts}
\usepackage{dsfont}
\usepackage{graphicx}
\graphicspath{{./}{qs/}}
\usepackage{color}
\usepackage[verbose,letterpaper,top=1.25in,bottom=1in,left=1.25in,right=1.25in]{geometry}
\usepackage{lastpage}
\usepackage{sgamevar}

\setlength{\parindent}{0pt}
\setlength{\parskip}{\baselineskip}

\newcounter{totalpoints}
\setcounter{totalpoints}{0}

\newcommand{\question}[2][]{(\textbf{#2}; \subtotal{#1})}
\newcommand{\subtotal}[1]{\newcounter{#1}\setcounter{#1}{0}\regtotcounter{#1}\total{#1} points}
\newcommand{\points}[2][]{{\addtocounter{totalpoints}{#2}\ifx&#1&\else\addtocounter{#1}{#2}\fi\textbf{[#2 points]}}}

\newcommand{\qsFile}[1]{}
\newcommand{\ansFile}[1]{}

\usepackage{environ}
\usepackage{etoolbox}
\makeatletter
\NewEnviron{answer}[1]
{\ifx\BODY\@empty
\vspace{#1}%
\else\ifdefined\noanswers
\vspace{#1}%
\else
{\sf\color{blue} \BODY}%
\fi\fi}
\makeatother

\usepackage{totcount}
\regtotcounter{totalpoints}
\begin{document}

{\bigskip\hrule\bigskip
\huge
\noindent CMPUT 261, Fall 2023\\
Assignment \#\assignNum{}

\large
Due: \duedate{}\\
Total points: \total{totalpoints}

For this assignment use the following consultation model:
\begin{enumerate}

\item you can discuss assignment questions and exchange ideas with other \emph{current} CMPUT~261 students;

\item you must list all members of the discussion in your solution;

\item you may {\bf not} share/exchange/discuss written material and/or code;

\item you must write up your solutions individually;

\item you must fully understand and be able to explain your solution in any amount of detail as requested by the instructor and/or the TAs.

\end{enumerate}

Anything that you use in your work and that is not your own creation must be properly cited by listing the original source. Failing to cite others' work is plagiarism and will be dealt with as an academic offence.


\bigskip\bigskip\hrule\bigskip

\vspace{1cm}
\hspace{1cm}{\bf First name:} \underline{\hspace{7cm}}

\vspace{1cm}
\hspace{1cm}{\bf Last name:} \underline{\hspace{7cm}}

\vspace{1cm}
\hspace{1cm}{\bf CCID:} \underline{\hspace{5.5cm}}\verb|@ualberta.ca|

\vspace{1cm}
\bigskip\hrule\bigskip
}

\pagestyle{myheadings}
\markboth{}{CMPUT 261 --- Assignment \#\assignNum{}}

\clearpage




\begin{enumerate}





%------------------------------------------------------------------------------------------
\item \question[mdp]{Markov Decision Processes}

Two coins are placed in a tray; each coin is either heads up or tails up with equal probability, independent of the other coin.  A robot arm starts above \textbf{one} of the coins, again with equal probability.  At each time step, the arm can perform one of the following operations:
\begin{itemize}
\item \textsf{Flip} the coin that it is above: The coin under the arm will be randomly set to either heads up or tails up with equal probability, and a reward of $-1$ is generated.
\item \textsf{Move} to be above the other coin; this generates a reward of $-2$.
\item \textsf{Call} for evaluation; a reward of $15$ is generated if both coins are heads up; a reward of $10$ is generated if both coins are tails up; or a reward of $-8$ if the coins mismatch.  After the reward is generated, both coins are flipped.
\end{itemize}
\begin{answer}{0in}
    % Answer here
\end{answer}
\begin{enumerate}
\item\points[mdp]{10} \label{q:mdp}
Represent this scenario formally as a Markov Decision Process.

\begin{answer}{2.5in}
    % Answer here
\end{answer}

\item\points[mdp]{2}
Is this a continuing or episodic scenario?  Justify your answer.
\begin{answer}{.75in}
    % Answer here
\end{answer}

\item\points[mdp]{3}
If you answered episodic to the previous question, describe how to treat the scenario as continuing.
If you answered continuing to the previous question, describe how to treat the scenario as episodic.
\begin{answer}{1in}
    % Answer here
\end{answer}
\end{enumerate}

\clearpage



%------------------------------------------------------------------------------------------
\item \question[bellman-q]{Bellman Action-Value Equation}
\begin{enumerate}
\item\points[bellman-q]{4}
Give an expression for $q_\pi(s,a)$ as an expectation of random variables, in the same form as equation (4.3) in the text Sutton \& Barto, \emph{Reinforcement Learning: An Introduction, 2nd edition.}
\begin{answer}{.5in}
    % Answer here
\end{answer}

\item\points[bellman-q]{4}
Give an expression for $q_\pi(s,a)$ as a weighted sum, in the same form as equation (4.4) in the text
Sutton \& Barto, \emph{Reinforcement Learning: An Introduction, 2nd edition.}
\begin{answer}{.5in}
    % Answer here
\end{answer}
\end{enumerate}



%------------------------------------------------------------------------------------------
\item \question[policy-improvement]{Policy Improvement}
Consider the following MDP with actions $\mathcal{A}=\{a,b\}$, states $\mathcal{S}=\{W,X,Y,Z\}$, and the following dynamics.  All unspecified transitions have probability 0:

\begin{align*}
p(X,4|W,a) &=0.5  & p(Z,0|X,a) &= 0.8    &  p(Z,1|Y,a) &=1 \\
p(Y,2|W,a) &=0.5  & p(Z,25|X,a) &=0.2   &  p(Z,5|Y,b) &=1 \\
p(Z,3|W,b) &= 1   & p(Z,0|X,b) &= 1      &  p(Z,0|Z,a) &=1 \\
&      &            &         &  p(Z,0|Z,b) &=1\\
\end{align*}

\begin{enumerate}
\item\points[policy-improvement]{4}
Consider the policy $\pi(b|s) = .5, \pi(a|s)=.5$ for all states $s\in\mathcal{S}$; i.e., a policy that randomizes between the two actions uniformly.
Using a discount rate of $\gamma=0.8$, what is the value $v_\pi(s)$ for each state $s\in\mathcal{S}$?
(Hint: What must the value of state $Z$ be under any policy?)
\begin{answer}{1.5in}
    % Answer here
\end{answer}

\item\points[policy-improvement]{8}
Construct a policy $\pi'$ that strictly improves upon $\pi$: That is, $v_{\pi'}(s) \ge v_\pi(s)$ for all $s\in\mathcal{S}$, and $v_{\pi'}(s) > v_\pi(s)$ for at least one $s \in \mathcal{S}$.  Appeal to the Policy Improvement Theorem to show that your new policy is indeed a strict improvement.
\begin{answer}{1.5in}
    % Answer here
\end{answer}
\end{enumerate}

\clearpage



%------------------------------------------------------------------------------------------
\item \question[mc-prediction]{Monte Carlo Prediction}
Consider an MDP with actions $\mathcal{A}=\{a,b,c\}$, states $\mathcal{S}=\{W,X,Y,Z\}$ (with terminal state $Z$), and unknown dynamics.
Suppose that you have used a policy $\pi$ to generate 4 episodes with the following trajectories:
\begin{align*}
&\langle S_0=W, A_0=a, R_1=0, S_1=X, A_1=a, R_2=10, S_2=Y, A_2=b, R_3=0, S_3=Z\rangle,\\
&\langle S_0=W, A_0=a, R_1=-10, S_1=X, A_1=b, R_2=0, S_2=Z\rangle,\\
&\langle S_0=W, A_0=b, R_1=2, S_1=Y, A_1=c, R_2=6, S_2=Z\rangle,\\
&\langle S_0=W, A_0=b, R_1=0, S_1=Y, A_1=c, R_2=12, S_2=Z\rangle.
\end{align*}
\begin{enumerate}
\item \points[mc-prediction]{8}
Use first-visit Monte Carlo prediction to estimate $v_\pi(s)$ for every $s\in\mathcal{S}$.  Assume undiscounted rewards (i.e., $\gamma=1$).

\begin{answer}{1.5in}
    % Answer here
\end{answer}
\end{enumerate}



%------------------------------------------------------------------------------------------
\item \question[td-q]{Temporal Difference Control}
Consider an MDP with actions $\mathcal{A}=\{a,b,c\}$, states $\mathcal{S}=\{W,X,Y\}$, and unknown dynamics.

Suppose that you have previously computed the following estimated action values:
\begin{align*}
Q(W,a) &= 0  &  Q(X,a) &= 8   &      Q(Y,a) &= 0  \\
Q(W,b) &= 0  &  Q(X,b) &= 7   &      Q(Y,b) &= 0  \\
Q(W,c) &= 0  &  Q(X,c) &= 16  &      Q(Y,c) &= 0  \\
\end{align*}

Now suppose that starting from state $S_t=W$, the current behaviour policy selects action $A_{t}=a$, leading to reward $R_{t+1}=4$ and a transition to state $S_{t+1}=X$.  The behaviour policy then selects action $A_{t+1}=a$.

\begin{enumerate}
\item \points[td-q]{6}
What is the updated estimate for $Q(W,a)$ according to the $Q$-learning algorithm?
Assume a step size of $\alpha=0.5$ and a discount rate of $\gamma=1$.
\begin{answer}{1.5in}
    % Answer here
\end{answer}

\item \points[td-q]{6}
What is the updated estimate for $Q(W,a)$ according to the Sarsa algorithm?
Assume a step size of $\alpha=0.5$ and a discount rate of $\gamma=1$.
\begin{answer}{1.5in}
    % Answer here
\end{answer}

\end{enumerate}





%------------------------------------------------------------------------------------------
\item \question[code]{Implementation}
\label{q:code}
Consider the following task (pictured below): an intrepid adventurer must make his way through a maze. To do so, he must first collect  a key, located somewhere in the maze, before leaving through a door. Without the key, the door will not open. Additionally, the maze contains dangerous fire hazards, which the adventurer wishes to avoid.

\begin{center}
\includegraphics[width=0.5\columnwidth]{code-qlearning.png}
\end{center}

This environment can be modelled as an MDP: the states correspond to the adventurer's $(x,y)$ position in the maze and whether he currently possesses the key. Rewards are 0 on every time step the agent has not reached the end with the key, 1 for successfully completing the maze, and -100 if the agent touches fire. The agent would like to finish the maze as quickly as possible, so the environment's discount rate is $\gamma = 0.95$.

This question requires several external Python libraries.  You can install them using the command
\begin{verbatim}
pip3 install --user -r requirements.txt
\end{verbatim}
from the assignment directory.

\begin{enumerate}
\item \points[code]{40}
Implement Q-learning by completing the implementations of the functions in the \verb|agent.py| source file.  An implementation of the environment, main training loop, and an action-value table are provided.
The agent should use an epsilon-greedy behavior policy to ensure adequate exploration of the maze. The constants $\gamma$, $\epsilon$ and step size are already given as attributes of the agent, please use these in your implementation.

We have provided public test cases for this assignment; run them using \verb|python3 tests.py|.
(These tests take approximately 4 minutes to run on my laptop.)

We will test your program by running (a copy of) the module defined in \verb|main.py|.  You can run the same program using \verb|python3 main.py|.  An automated graphic will display a final episode after training has been completed, and a learning curve, showing the length of episodes over time will be generated in \verb|learning_curve.png|.

\clearpage
\item \points[code]{3}
In the function \verb|display_final_episode()| the agent is evaluated using a \textit{greedy} policy, instead of an $\epsilon$-greedy policy.  Why does it make sense to evaluate the agent this way? Would this still work with Sarsa?
\begin{answer}{1.5in}
    % Answer here
\end{answer}

\item \points[code]{3}
Could Monte Carlo control be used instead to solve this problem? Justify your answer.
\begin{answer}{1.5in}
    % Answer here
\end{answer}
\end{enumerate}

\end{enumerate}


\clearpage

\section*{Submission}
The assignment you downloaded from eClass is a single ZIP archive which includes this document as a PDF {\em and} its \LaTeX{} source as well as a Python file needed for Question~\ref{q:code}.
You are to unzip the archive into an empty directory, work on the problems and then zip the directory into a new single ZIP archive for submission.

\medskip

Each assignment is to be submitted electronically via eClass by the due date.
\textbf{Your submission must be a single ZIP file containing}:
\begin{enumerate}
\item a single PDF file with your answers;
\item file(s) with your Python code.
\end{enumerate}

To generate the PDF file with your answers you can do any of the following:

\begin{itemize}
\item
insert your answers into the provided \LaTeX{} source file between \verb|\begin{answer}| and \verb|\end{answer}|. Then run the source through \LaTeX{} to produce a PDF file;

\item print out the provided PDF file and legibly write your answers in the blank spaces under each question. Make sure you write as legibly as possible for we cannot give you any points if we cannot read your hand-writing. Then scan the pages and include the scan in your ZIP submission to be uploaded on eClass;

\item use your favourite text processor and type up your answers there. Make sure you number your answers in the same way as the questions are numbered in this assignment.
\end{itemize}

\end{document}

\newpage


\end{enumerate}

\section*{Submission}
The assignment you downloaded from eClass is a single ZIP archive which includes this document as a PDF {\em and} its \LaTeX{} source as well as code.

\medskip

Each assignment is to be submitted electronically via eClass by the due date.
\textbf{Your submission must be a zip containing your answers and code. You must name your zip file with your CCID. } This zip should only contain your PDF with answers and the q6 directory. Please do not change the structure of q6 directory within your submission.

To generate the PDF file with your answers you can do any of the following:

\begin{itemize}
\item
insert your answers into the provided \LaTeX{} source file between \verb|\begin{answer}| and \verb|\end{answer}|. Then run the source through \LaTeX{} to produce a PDF file;

\item print out the provided PDF file and legibly write your answers in the blank spaces under each question. Make sure you write as legibly as possible for we cannot give you any points if we cannot read your hand-writing. Then scan the pages and include the scan in your ZIP submission to be uploaded on eClass;

\item use your favourite text processor and type up your answers there. Make sure you number your answers in the same way as the questions are numbered in this assignment.
\end{itemize}



\end{document}
